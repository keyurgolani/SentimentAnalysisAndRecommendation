{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# MOVIE REVIEWS SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Python 3\n",
    "import re #Please check that you have these!\n",
    "from os import listdir\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np #Only end of recommendation system is affected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt #Only plotting will not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Work with a single review"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Movie reviews are taken and adapted from \"Large Movie Review Dataset v1.0\":\n",
    "http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "Folders structure:\n",
    "Practical_NLP\n",
    "|-neg                     #Folder contains all negative reviews\n",
    "   |-0_3_tt0064354.txt    #File name format: [revID]_[score]_[movID].txt\n",
    "   |-1_1_tt0100680.txt\n",
    "   |-\n",
    "|-pos                     #Folder contains all positive revies\n",
    "   |-0_9_tt0453418.txt\n",
    "   |-1_7_tt0453418.txt\n",
    "   |-...\n",
    "|-polarity_words.csv      #File format: word,polarity\n",
    "\n",
    "|-Recommendation.ipynb\n",
    "|-Recommendation.py\n",
    "|-Sentiment_analysis.ipynb\n",
    "|-Sentiment_analysis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fair drama/love story movie that focuses on the lives of blue collar people finding new life thru new love.The acting here is good but the film fails in cinematography,screenplay,directing and editing.The story/script is only average at best.This film will be enjoyed by Fonda and De Niro fans and by people who love middle age love stories where in the coartship is on a more wiser and cautious level.It would also be interesting for people who are interested on the subject matter regarding illiteracy.......\n"
     ]
    }
   ],
   "source": [
    "#Open and output each line of a review\n",
    "file1 = 'pos/6_10_tt0100680.txt'\n",
    "f = open(file1,'r')\n",
    "for line in f:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['De', 'Fair', 'Fonda', 'Niro', 'a', 'acting', 'age', 'also', 'and', 'and', 'and', 'and', 'are', 'at', 'average', 'be', 'be', 'best.This', 'blue', 'but', 'by', 'by', 'cautious', 'cinematography,screenplay,directing', 'coartship', 'collar', 'drama/love', 'editing.The', 'enjoyed', 'fails', 'fans', 'film', 'film', 'finding', 'focuses', 'for', 'good', 'here', 'illiteracy.......', 'in', 'in', 'interested', 'interesting', 'is', 'is', 'is', 'level.It', 'life', 'lives', 'love', 'love', 'love.The', 'matter', 'middle', 'more', 'movie', 'new', 'new', 'of', 'on', 'on', 'on', 'only', 'people', 'people', 'people', 'regarding', 'stories', 'story', 'story/script', 'subject', 'that', 'the', 'the', 'the', 'the', 'thru', 'where', 'who', 'who', 'will', 'wiser', 'would']\n"
     ]
    }
   ],
   "source": [
    "f = open(file1,'r')\n",
    "words = [] #List of all words in a review\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    words = line.split() #Split by spaces\n",
    "    \n",
    "print(sorted(words)) #remained \"word.\" or \"word:\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', 'De', 'Fair', 'Fonda', 'It', 'Niro', 'The', 'The', 'This', 'a', 'acting', 'age', 'also', 'and', 'and', 'and', 'and', 'are', 'at', 'average', 'be', 'be', 'best', 'blue', 'but', 'by', 'by', 'cautious', 'cinematography,screenplay,directing', 'coartship', 'collar', 'drama', 'editing', 'enjoyed', 'fails', 'fans', 'film', 'film', 'finding', 'focuses', 'for', 'good', 'here', 'illiteracy', 'in', 'in', 'interested', 'interesting', 'is', 'is', 'is', 'level', 'life', 'lives', 'love', 'love', 'love', 'love', 'matter', 'middle', 'more', 'movie', 'new', 'new', 'of', 'on', 'on', 'on', 'only', 'people', 'people', 'people', 'regarding', 'script', 'stories', 'story', 'story', 'subject', 'that', 'the', 'the', 'the', 'the', 'thru', 'where', 'who', 'who', 'will', 'wiser', 'would']\n"
     ]
    }
   ],
   "source": [
    "f = open(file1,'r')\n",
    "words = []\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    words = re.split(' |, |: |!|\\.|\"|\\'|\\(|\\)|\\?|/|;|>|<',line) \n",
    "    #Split review using all possible punctuations as delimiters\n",
    "    # \"|\" is used to show where each delimiter ends\n",
    "\n",
    "print(sorted(words)) #Same words, inneficient, better a dictionary with a word and its count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'acting', 'age', 'also', 'and', 'are', 'at', 'average', 'be', 'best', 'blue', 'but', 'by', 'cautious', 'cinematography,screenplay,directing', 'coartship', 'collar', 'de', 'drama', 'editing', 'enjoyed', 'fails', 'fair', 'fans', 'film', 'finding', 'focuses', 'fonda', 'for', 'good', 'here', 'illiteracy', 'in', 'interested', 'interesting', 'is', 'it', 'level', 'life', 'lives', 'love', 'matter', 'middle', 'more', 'movie', 'new', 'niro', 'of', 'on', 'only', 'people', 'regarding', 'script', 'stories', 'story', 'subject', 'that', 'the', 'this', 'thru', 'where', 'who', 'will', 'wiser', 'would']\n",
      "The vocabulary contains  65  words.\n"
     ]
    }
   ],
   "source": [
    "f = open(file1,'r')\n",
    "vocab = {} #Dictionary: {word1:count1, word2:count2,...}\n",
    "for line in f:\n",
    "    line = line.strip().lower()\n",
    "    words = re.split(' |, |: |!|\\.|\"|\\'|\\(|\\)|\\?|/|;|>|<',line)\n",
    "    for word in words:\n",
    "        if word == '': #No need to have empty strings in the vocabulary\n",
    "            continue\n",
    "        vocab[word]=vocab.get(word,0)+1 #Increment the count of the just seen word\n",
    "\n",
    "print(sorted(vocab))\n",
    "vLen = len(vocab) #Length of vocabulary\n",
    "print(\"The vocabulary contains \",vLen,\" words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#The above procedure can be used done with NLTK module\n",
    "from nltk.tokenize import word_tokenize #Load NLTK word_tokenize module\n",
    "#If you got an error - just watch the explanation of the next cell\n",
    "#Resume the work after the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'tokenizers/punkt/PY3/english.pickle' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/Users/keyurgolani/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-132b0f3a6b07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Instead of regular expressions use this to tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/keyurgolani/anaconda/envs/NLP/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \"\"\"\n\u001b[0;32m--> 109\u001b[0;31m     return [token for sent in sent_tokenize(text, language)\n\u001b[0m\u001b[1;32m    110\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/keyurgolani/anaconda/envs/NLP/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/keyurgolani/anaconda/envs/NLP/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/keyurgolani/anaconda/envs/NLP/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/keyurgolani/anaconda/envs/NLP/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'tokenizers/punkt/PY3/english.pickle' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/Users/keyurgolani/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "f = open(file1,'r')\n",
    "vocab = {}\n",
    "for line in f:\n",
    "    line = line.strip().lower()\n",
    "    words = word_tokenize(line.strip().lower()) #Instead of regular expressions use this to tokenize\n",
    "    for word in words:\n",
    "        if word == '':\n",
    "            continue\n",
    "        vocab[word]=vocab.get(word,0)+1\n",
    "\n",
    "print(sorted(vocab))\n",
    "vLen = len(vocab)\n",
    "print(\"The vocabulary contains \",vLen,\" words.\") \n",
    "#Vocabulary length is different as word_tokenize also included \",\",\".\",\"...\", \n",
    "#but did not tokenize \"story/script\", \"best.this\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.bar(range(len(vocab)), vocab.values()) #Show counts for each word in our dictionary\n",
    "plt.show() #Most counts are 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "top20_vocab = dict(sorted(vocab.items(), key=lambda x: -x[1])[:20]) #Dictionary of the words with the highest count\n",
    "top20_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Bar chart of 20 words with the highest count\n",
    "plt.bar(range(len(top20_vocab)), top20_vocab.values(), align='center')\n",
    "plt.xlim([-1,20])\n",
    "plt.xticks(range(len(top20_vocab)), top20_vocab.keys(), rotation=90)\n",
    "plt.show() #Most polarized words (important for classification) as \"good\" appear only 1 time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load polarity words, adapted from http://sentiwordnet.isti.cnr.it/\n",
    "f = open('polarity_words_uniq.csv','r')\n",
    "i = 0\n",
    "pol_words = {} #Dictionary of polarity words: {pol_word1:polarity1, pol_word2:polarity2,...}\n",
    "next(f) #Skip header\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    line = line.split(',')\n",
    "    pol_words[line[0]] = np.sign(float(line[1])) #+1 is for positive words, -1 is for negative words\n",
    "pol_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pol_words.get('and',None),pol_words.get('love',None),pol_words.get('interesting',None) #Check polarity of several words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "words = top20_vocab.keys() #List of top20 words\n",
    "counts = top20_vocab.values() #their counts\n",
    "pos = range(len(top20_vocab)) #their positions on a bar chart\n",
    "barWords= plt.bar(pos, counts, align='center', color='w') #make bar chart of counts for all top20 words\n",
    "for i,word in enumerate(words):\n",
    "    polarity = pol_words.get(word,None) #Get polarity of each word\n",
    "    if polarity==1:\n",
    "        barWords[i].set_color('r') #Change the color to \"red\" for positive word\n",
    "    if polarity==-1:\n",
    "        barWords[i].set_color('b') #Change the color to \"blue\" for negative word\n",
    "plt.xlim([-1,20]) #Set the range to display\n",
    "plt.xticks(pos, words, rotation=90) #Add words to the ticks on X axis\n",
    "plt.show() #Most polarized words (important for classification) as \"good\" appear only 1 time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Naive Bayes model:\n",
    "# Type of review is defined by:\n",
    "# the number of positive words - number of negative words\n",
    "# If there are more positive words --> positive review\n",
    "# otherwise --> negative review\n",
    "vote = 0\n",
    "for word in vocab.keys():\n",
    "    polarity = pol_words.get(word,0)\n",
    "    vote += polarity\n",
    "if vote>0:\n",
    "    print(\"Positive review, vote =\",vote)\n",
    "elif vote == 0:\n",
    "    print(\"Neutral review\") #If the number of positive words == number of negative words\n",
    "else:\n",
    "    print(\"Negative review, vote =\",vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The above analysis is combined in this cell.\n",
    "# For a single review\n",
    "file1 = 'pos/6_10_tt0100680.txt' #Check other \n",
    "f = open(file1,'r')\n",
    "\n",
    "vocab = {}\n",
    "for line in f:\n",
    "    line = line.strip().lower()\n",
    "    words = re.split(' |, |: |!|\\.|\"|\\(|\\)|\\?|/|;|>|<',line)\n",
    "    for word in words:\n",
    "        if word == '':\n",
    "            continue\n",
    "        vocab[word]=vocab.get(word,0)+1\n",
    "\n",
    "vLen = len(vocab)\n",
    "top20_vocab = dict(sorted(vocab.items(), key=lambda x: -x[1])[:20])\n",
    "\n",
    "counts = top20_vocab.values()\n",
    "words = top20_vocab.keys()\n",
    "pos = range(len(top20_vocab))\n",
    "barWords= plt.bar(pos, counts, align='center', color='w')\n",
    "for i,word in enumerate(words):\n",
    "    polarity = pol_words.get(word,None)\n",
    "    if polarity==1:\n",
    "        barWords[i].set_color('r') #Change the color of only the i-th bar\n",
    "    if polarity==-1:\n",
    "        barWords[i].set_color('b')\n",
    "plt.xlim([-1,20])\n",
    "plt.xticks(pos, words, rotation=90)\n",
    "plt.show() #Most polarized words (important for classification) as \"good\" appear only 1 time\n",
    "\n",
    "vote = 0\n",
    "for word in vocab.keys():\n",
    "    polarity = pol_words.get(word,0)\n",
    "    vote += polarity\n",
    "if vote>0:\n",
    "    print(\"Positive review, vote =\",vote)\n",
    "elif vote == 0:\n",
    "    print(\"Neutral review\")\n",
    "else:\n",
    "    print(\"Negative review, vote =\",vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#For a single review with NLTK\n",
    "file1 = 'pos/6_10_tt0100680.txt'\n",
    "f = open(file1,'r')\n",
    "\n",
    "vocab = {}\n",
    "\n",
    "for line in f:\n",
    "    line = line.strip().lower()\n",
    "    words = word_tokenize(line.strip().lower())\n",
    "    for word in words:\n",
    "        if word == '':\n",
    "            continue\n",
    "        vocab[word]=vocab.get(word,0)+1\n",
    "\n",
    "vLen = len(vocab)\n",
    "top20_vocab = dict(sorted(vocab.items(), key=lambda x: -x[1])[:20])\n",
    "\n",
    "counts = top20_vocab.values()\n",
    "words = top20_vocab.keys()\n",
    "pos = range(len(top20_vocab))\n",
    "barWords= plt.bar(pos, counts, align='center', color='w')\n",
    "for i,word in enumerate(words):\n",
    "    polarity = pol_words.get(word,None)\n",
    "    if polarity==1:\n",
    "        barWords[i].set_color('r') #Change the color of only the i-th bar\n",
    "    if polarity==-1:\n",
    "        barWords[i].set_color('b')\n",
    "plt.xlim([-1,20])\n",
    "plt.xticks(pos, words, rotation=90)\n",
    "plt.show() #Most polarized words (important for classification) as \"good\" appear only 1 time\n",
    "\n",
    "vote = 0\n",
    "for word in vocab.keys():\n",
    "    polarity = pol_words.get(word,0)\n",
    "    vote += polarity\n",
    "if vote>0:\n",
    "    print(\"Positive review, vote =\",vote)\n",
    "elif vote == 0:\n",
    "    print(\"Neutral review\")\n",
    "else:\n",
    "    print(\"Negative review, vote =\",vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Classify all reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Using regular expressions for tokenization\n",
    "fold = 'pos/'\n",
    "n_pos = 0 #Number of positive reviews\n",
    "n_files = 0 #Total number of files\n",
    "for file in listdir(fold): #Get the name of each file in fold\n",
    "    n_files += 1\n",
    "    if n_files>5: #Comment this\n",
    "        break     #Comment this\n",
    "    file1 = fold + file\n",
    "    vote = 0 #The total vote for each review\n",
    "    try:\n",
    "        f = open(file1,'r')\n",
    "        for line in f:\n",
    "            print(line) #Comment this\n",
    "            line = line.strip().lower()\n",
    "            words = re.split(' |, |: |!|\\.|\"|\\'|\\(|\\)|\\?|/|;|>|<',line)\n",
    "            for word in words:\n",
    "                polarity = pol_words.get(word,0)\n",
    "                vote += polarity\n",
    "        print('\\n') #Comment this\n",
    "        if vote>0:\n",
    "            print(\"Positive review, vote =\",vote) #Comment this\n",
    "            n_pos += 1\n",
    "        elif vote == 0:\n",
    "            print(\"Neutral review\") #Comment this\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Negative review, vote =\",vote) #Comment this\n",
    "            pass\n",
    "        print('\\n') #Comment this\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "#Check the 4th review with the vote -14. It is indeed difficult to say from the text that this is a positive review.\n",
    "print(\"Classifier accuracy is\",n_pos/n_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
